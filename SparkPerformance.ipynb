{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying the performance of Spark\n",
    "\n",
    "### Students:\n",
    "- Kawtar LAGHDAF\n",
    "- Tijana NINKOVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[3]\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Spark and Pandas\n",
    "\n",
    "We chose to study Spark DataFrames and compare them to the Pandas library.<br>\n",
    "A few key differences between Spark DataFrames and Pandas are:\n",
    "> Pandas data frame is not distributed and Spark DataFrame is. Therefore, when working with large amounts of data, Spark will outperform Pandas.\n",
    "<br><br>\n",
    "> Spark DataFrame is resilient and assures fault tolerance, whereas Pandas data frame does not. This is because in case of failure Spark can regenerate the failed result from lineage. Pandas does not have this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's DataFrames are designed for processing large amounts of structured or semi-structured data.\n",
    "It has the ability to handle petabytes of data. It has support for wide range of data formats and sources.\n",
    "<br>\n",
    "DataFrames are distributed which makes them highly available and fault tolerant. They also have lazy evaluation like RDDs, meaning that execution won't start until an action is triggered. DataFrames are also immutable, which means that once created, they cannot be changed. The values can, however, be transformed by applying transformations, just like in RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at how similar these two libraries are by doing some simple analysis in both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------------------+------------+---------------+\n",
      "|timestamp|machine_ID|event_type|         platform_ID|capacity_CPU|capacity_memory|\n",
      "+---------+----------+----------+--------------------+------------+---------------+\n",
      "|        0|         5|         0|HofLGzk1Or/8Ildj2...|         0.5|         0.2493|\n",
      "|        0|         6|         0|HofLGzk1Or/8Ildj2...|         0.5|         0.2493|\n",
      "|        0|         7|         0|HofLGzk1Or/8Ildj2...|         0.5|         0.2493|\n",
      "|        0|        10|         0|HofLGzk1Or/8Ildj2...|         0.5|         0.2493|\n",
      "|        0|        13|         0|HofLGzk1Or/8Ildj2...|         0.5|         0.2493|\n",
      "+---------+----------+----------+--------------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machine_events_header = [\"timestamp\", \"machine_ID\", \"event_type\", \"platform_ID\", \"capacity_CPU\", \"capacity_memory\"]\n",
    "#Loading data set with Spark DataFrame with read.csv\n",
    "machine_events_spark = spark.read.csv('clusterdata-2011-2/machine_events/part-00000-of-00001.csv.gz')\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c0\", \"timestamp\")\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c1\", \"machine_ID\")\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c2\", \"event_type\")\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c3\", \"platform_ID\")\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c4\", \"capacity_CPU\")\n",
    "machine_events_spark = machine_events_spark.withColumnRenamed(\"_c5\", \"capacity_memory\")\n",
    "machine_events_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>machine_ID</th>\n",
       "      <th>event_type</th>\n",
       "      <th>platform_ID</th>\n",
       "      <th>capacity_CPU</th>\n",
       "      <th>capacity_memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  machine_ID  event_type  \\\n",
       "0          0           5           0   \n",
       "1          0           6           0   \n",
       "2          0           7           0   \n",
       "3          0          10           0   \n",
       "4          0          13           0   \n",
       "\n",
       "                                    platform_ID  capacity_CPU  capacity_memory  \n",
       "0  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=           0.5           0.2493  \n",
       "1  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=           0.5           0.2493  \n",
       "2  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=           0.5           0.2493  \n",
       "3  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=           0.5           0.2493  \n",
       "4  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=           0.5           0.2493  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data set with pandas with read_csv\n",
    "machine_events_pandas = pd.read_csv('clusterdata-2011-2/machine_events/part-00000-of-00001.csv.gz',\n",
    "                            names = machine_events_header )\n",
    "machine_events_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37780"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of rows with Spark\n",
    "machine_events_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp          37780\n",
       "machine_ID         37780\n",
       "event_type         37780\n",
       "platform_ID        37780\n",
       "capacity_CPU       37748\n",
       "capacity_memory    37748\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of rows with Pandas\n",
    "machine_events_pandas.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtainted are slightly different, because Pandas counts for each of the columns separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35015"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering with Spark\n",
    "machine_events_spark.filter(machine_events_spark.capacity_CPU == '0.5').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp          35015\n",
       "machine_ID         35015\n",
       "event_type         35015\n",
       "platform_ID        35015\n",
       "capacity_CPU       35015\n",
       "capacity_memory    35015\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering with Pandas\n",
    "machine_events_pandas[machine_events_pandas['capacity_CPU']==0.5].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|corr(capacity_CPU, capacity_memory)|\n",
      "+-----------------------------------+\n",
      "|                 0.6848616088591185|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding correlation with Spark\n",
    "from pyspark.sql.functions import corr\n",
    "machine_events_spark.select(corr(\"capacity_CPU\",\"capacity_memory\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capacity_CPU</th>\n",
       "      <th>capacity_memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>capacity_CPU</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capacity_memory</th>\n",
       "      <td>0.684862</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 capacity_CPU  capacity_memory\n",
       "capacity_CPU         1.000000         0.684862\n",
       "capacity_memory      0.684862         1.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding correlation with Pandas\n",
    "machine_events_pandas[['capacity_CPU','capacity_memory']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing some similar functions of both libraries, we can look at their performance and compare it.\n",
    "<br>For this we will use **SQL queries** in Spark and equivalent operations in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4052718620000633\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "machine_events_spark.registerTempTable('machines_spark')\n",
    "spark.sql('select * from machines_spark where capacity_memory > 0.4 and capacity_CPU == 0.5').collect()\n",
    "end = timer()\n",
    "spark_time_select = end - start\n",
    "print(spark_time_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005688225999847418\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_pandas[(machine_events_pandas['capacity_CPU'] == 0.5) & (machine_events_pandas['capacity_memory']>0.4)]\n",
    "end = timer()\n",
    "pandas_time_select = end - start\n",
    "print(pandas_time_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comparison will be **sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.989183776000118\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_spark.registerTempTable('machines_spark')\n",
    "spark.sql('select * from machines_spark order by capacity_CPU, capacity_memory').collect()\n",
    "end = timer()\n",
    "spark_time_sort = end - start\n",
    "print(spark_time_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18032287000005454\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_pandas.sort_values(by=['capacity_CPU', 'capacity_memory'])\n",
    "end = timer()\n",
    "pandas_time_sort = end - start\n",
    "print(pandas_time_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will be **joining** datasets. We will join machine events with machine attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3305594270000256\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_attributes_spark = spark.read.csv('clusterdata-2011-2/machine_attributes/part-00000-of-00001.csv.gz')\n",
    "machine_attributes_spark = machine_attributes_spark.withColumnRenamed(\"_c0\", \"timestamp\")\n",
    "machine_attributes_spark = machine_attributes_spark.withColumnRenamed(\"_c1\", \"machine_ID\")\n",
    "machine_attributes_spark = machine_attributes_spark.withColumnRenamed(\"_c2\", \"attribute_name\")\n",
    "machine_attributes_spark = machine_attributes_spark.withColumnRenamed(\"_c3\", \"attribute_value\")\n",
    "machine_attributes_spark = machine_attributes_spark.withColumnRenamed(\"_c4\", \"attribute_deleted\")\n",
    "end = timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.92662312900006\n"
     ]
    }
   ],
   "source": [
    "machine_attributes_header = [\"timestamp\", \"machine_ID\", \"attribute_name\", \"attribute_value\", \"attribute_deleted\"]\n",
    "start = timer()\n",
    "machine_attributes_pandas = pd.read_csv('clusterdata-2011-2/machine_attributes/part-00000-of-00001.csv.gz',\n",
    "                            names = machine_attributes_header )\n",
    "end = timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.521929436999926\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_spark.registerTempTable('machines_spark')\n",
    "machine_attributes_spark.registerTempTable('machines2_spark')\n",
    "spark.sql('select * from machines_spark join machines2_spark on machines_spark.timestamp = machines2_spark.timestamp and machines_spark.machine_ID = machines2_spark.machine_ID').collect()\n",
    "end = timer()\n",
    "spark_time_join = end - start\n",
    "print(spark_time_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.256840640999144\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "pd.merge(machine_events_pandas, machine_attributes_pandas, how = 'left', left_on = ['timestamp', 'machine_ID'], right_on = ['timestamp', 'machine_ID'])\n",
    "end = timer()\n",
    "pandas_time_join = end - start\n",
    "print(pandas_time_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final thing we will compare is **grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9742381450014364\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_spark.registerTempTable('machines_spark')\n",
    "spark.sql('select count(*) from machines_spark group by capacity_CPU').collect()\n",
    "end = timer()\n",
    "spark_time_group = end - start\n",
    "print(spark_time_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020762589992955327\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "machine_events_pandas.groupby('capacity_CPU').size()\n",
    "end = timer()\n",
    "pandas_time_group = end - start\n",
    "print(pandas_time_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Spark</th>\n",
       "      <th>Pandas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>where clauses</td>\n",
       "      <td>0.517818</td>\n",
       "      <td>0.004334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sorting</td>\n",
       "      <td>1.117553</td>\n",
       "      <td>0.009236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joining</td>\n",
       "      <td>37.521929</td>\n",
       "      <td>4.256841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grouping</td>\n",
       "      <td>0.974238</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analysis      Spark    Pandas\n",
       "0  where clauses   0.517818  0.004334\n",
       "1        sorting   1.117553  0.009236\n",
       "2        joining  37.521929  4.256841\n",
       "3       grouping   0.974238  0.002076"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Analysis', 'Spark', 'Pandas'])\n",
    "results_df = results_df.append({'Analysis': 'where clauses', 'Spark': spark_time_select, 'Pandas': pandas_time_select}, ignore_index=True)\n",
    "results_df = results_df.append({'Analysis': 'sorting', 'Spark': spark_time_sort, 'Pandas': pandas_time_sort}, ignore_index=True)\n",
    "results_df = results_df.append({'Analysis': 'joining', 'Spark': spark_time_join, 'Pandas': pandas_time_join}, ignore_index=True)\n",
    "results_df = results_df.append({'Analysis': 'grouping', 'Spark': spark_time_group, 'Pandas': pandas_time_group}, ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.bar(results_df['Analysis'], results_df['Spark'], label = \"Spark\", tick_label = results_df['Analysis'], width=0.3, align='edge', color='r')\n",
    "plt.bar(results_df['Analysis'], results_df['Pandas'], label = \"Pandas\", tick_label = results_df['Analysis'], width=-0.3, align='edge', color='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our results represented in a table and graphically, we can analyze them properly and draw conslusions. For these analysis we used very small data sets and we can see that in all of the analysis Pandas outperformed Spark. We conclude that for smaller datasets it is definitely better to use Pandas, because Spark's complexity slowed it down.\n",
    "\n",
    "Next, we will try to repeat these analysis, but with a bigger data set, where Spark is expected to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
